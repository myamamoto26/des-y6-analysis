{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be450006-c3fb-45c2-888f-c55156362bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitsio as fio\n",
    "import numpy as np\n",
    "import os, sys\n",
    "import glob\n",
    "from matplotlib import pyplot as plt\n",
    "import proplot as pplt\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from des_y6utils import mdet\n",
    "\n",
    "os.environ[\"MEDS_DIR\"] = \"/global/cfs/cdirs/des/y6-shear-catalogs\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f119a6ea-716e-4ad1-8f52-f366845a9429",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Accessing metadetection catalogs on NERSC. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b001b73-479b-4906-985a-dd4b8423f974",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab mdet files (these files are divided into patches)\n",
    "mdet_files = glob.glob('/global/cfs/cdirs/des/y6-shear-catalogs/Y6A2_METADETECT_V4/jackknife_pathces_blinded/patch-*.fits')\n",
    "# Set which measurement you want to use. \n",
    "# For catalogs in Y6A2_METADETECT_V3, use mdet_mom='wmom'\n",
    "# For catalogs in Y6A2_METADETECT_V4, use mdet_mom='gauss'\n",
    "# 'wmom' used weighted moments to compute shear, size etc. 'gauss' used gaussian fit. \n",
    "mdet_mom = 'gauss' \n",
    "    \n",
    "## NOTE ##\n",
    "# 1. We will have a new catalog which has a major update on the shear estimator. I will include the path to the latest catalog once it's ready. \n",
    "# 2. Catalogs for individual tiles exist in /global/cfs/cdirs/des/y6-shear-catalogs/Y6A2_METADETECT_V4/tiles_blinded. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04cb2fd5-f1b4-450f-98e6-5d84fea2bb40",
   "metadata": {
    "tags": []
   },
   "source": [
    "### shear weight that is binned by S/N and size ratio (similar to Y3)\n",
    "we don't use this anymore. See cell 7 for the new weight definition. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65987ade-b323-4806-bd2c-df4c74762d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preliminary shear weight computed in the bins of S/N and size ratio. Similar to Fig. 4 in Y3 catalog paper.\n",
    "# If you're using the cut ID = 2, you'd want to use inverse_variance_weight_final_v2.pickle, rather than v1. \n",
    "# with open('/global/cscratch1/sd/myamamot/des-y6-analysis/y6_measurement/v2/inverse_variance_weight_final_v2.pickle', 'rb') as handle:\n",
    "#     wgt_dict = pickle.load(handle)\n",
    "\n",
    "# # FOR 'WMOM' MEASUREMENT, USE\n",
    "# snmin = 10\n",
    "# snmax = 500\n",
    "# sizemin = 1.2\n",
    "# sizemax = 2.0\n",
    "# steps = 20\n",
    "\n",
    "# FOR 'GAUSS' MEASUREMENT, USE\n",
    "# snmin = 10\n",
    "# snmax = 500\n",
    "# sizemin = 0.5\n",
    "# sizemax = 3.0\n",
    "# steps = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3eb518a3-59b0-443a-a37c-acda4e9c8dc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "[('uid', '>i8'), ('patch_num', '>i2'), ('tilename', '<U12'), ('slice_id', '>i2'), ('mdet_step', '<U7'), ('ra', '>f8'), ('dec', '>f8'), ('x', '>f4'), ('y', '>f4'), ('mfrac', '>f4'), ('mfrac_img', '>f4'), ('nepoch_g', '>i4'), ('nepoch_r', '>i4'), ('nepoch_i', '>i4'), ('nepoch_z', '>i4'), ('psfrec_g_1', '>f8'), ('psfrec_g_2', '>f8'), ('psfrec_T', '>f4'), ('wmom_s2n', '>f4'), ('wmom_g_1', '>f8'), ('wmom_g_2', '>f8'), ('wmom_g_cov_1_1', '>f4'), ('wmom_g_cov_1_2', '>f4'), ('wmom_g_cov_2_2', '>f4'), ('wmom_T_err', '>f4'), ('wmom_T_ratio', '>f4'), ('wmom_psf_T', '>f4'), ('pgauss_T_err', '>f4'), ('pgauss_T', '>f4'), ('pgauss_psf_T', '>f4'), ('pgauss_band_flux_g', '>f4'), ('pgauss_band_flux_r', '>f4'), ('pgauss_band_flux_i', '>f4'), ('pgauss_band_flux_z', '>f4'), ('pgauss_band_flux_err_g', '>f4'), ('pgauss_band_flux_err_r', '>f4'), ('pgauss_band_flux_err_i', '>f4'), ('pgauss_band_flux_err_z', '>f4')]\n"
     ]
    }
   ],
   "source": [
    "print(len(mdet_files))\n",
    "d = fio.read(mdet_files[0])\n",
    "print(d.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8104e4-0c1e-4242-9f38-4260faff04a2",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Computing weighted shear response over all the catalogs. \n",
    "### Note that for metadetection we do not have per-object shear response. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79aa8443-f69d-405b-a5c8-21b5ef813383",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _accum_shear_per_tile(res, mdet_step, g1, g2, weight):\n",
    "\n",
    "    \"\"\"\n",
    "    Returns the dictionary of the accumulated shear (sum of individual shear).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    res: A dictionary in which accumulated sums of shear are stored\n",
    "    mdet_step: An array of metadetection steps (noshear, 1p, 1m, 2p, 2m) for each object in metadetection catalog\n",
    "    g1: An array of the measured shapes (e1) for each object in metadetection catalog\n",
    "    g2: An array of the measured shapes (e2) for each object in metadetection catalog\n",
    "    weight: Weight of each object\n",
    "    \"\"\"\n",
    "    for step in ['noshear', '1p', '1m', '2p', '2m']:\n",
    "        msk_s = np.where(mdet_step == step)[0]\n",
    "        \n",
    "        np.add.at(\n",
    "            res[step], \n",
    "            (0, 0), \n",
    "            np.sum(weight[msk_s] * g1[msk_s]),\n",
    "        )\n",
    "        np.add.at(\n",
    "            res[step], \n",
    "            (0, 1), \n",
    "            np.sum(weight[msk_s] * g2[msk_s]),\n",
    "        )\n",
    "        np.add.at(\n",
    "            res[\"num_\" + step], \n",
    "            (0, 0), \n",
    "            np.sum(weight[msk_s]),\n",
    "        )\n",
    "        np.add.at(\n",
    "            res[\"num_\" + step], \n",
    "            (0, 1), \n",
    "            np.sum(weight[msk_s]),\n",
    "        )\n",
    "    return res\n",
    "\n",
    "def assign_loggrid(x, y, xmin, xmax, xsteps, ymin, ymax, ysteps):\n",
    "    from math import log10\n",
    "    # return x and y indices of data (x,y) on a log-spaced grid that runs from [xy]min to [xy]max in [xy]steps\n",
    "\n",
    "    logstepx = log10(xmax/xmin)/xsteps\n",
    "    logstepy = log10(ymax/ymin)/ysteps\n",
    "\n",
    "    indexx = (np.log10(x/xmin)/logstepx).astype(int)\n",
    "    indexy = (np.log10(y/ymin)/logstepy).astype(int)\n",
    "\n",
    "    indexx = np.maximum(indexx,0)\n",
    "    indexx = np.minimum(indexx, xsteps-1)\n",
    "    indexy = np.maximum(indexy,0)\n",
    "    indexy = np.minimum(indexy, ysteps-1)\n",
    "\n",
    "    return indexx,indexy\n",
    "\n",
    "def _find_shear_weight(d, wgt_dict, snmin, snmax, sizemin, sizemax, steps, mdet_mom):\n",
    "    \n",
    "    if wgt_dict is None:\n",
    "        weights = np.ones(len(d))\n",
    "        return weights\n",
    "\n",
    "    shear_wgt = wgt_dict['weight']\n",
    "    shear_response = wgt_dict['response']\n",
    "    indexx, indexy = assign_loggrid(d[mdet_mom+'_s2n'], d[mdet_mom+'_T_ratio'], snmin, snmax, steps, sizemin, sizemax, steps)\n",
    "    weights = np.array([shear_wgt[x, y] for x, y in zip(indexx, indexy)])\n",
    "    # response = np.array([shear_response[x, y] for x, y in zip(indexx, indexy)])\n",
    "    \n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ba625ef-2109-44e7-a935-edf36625641c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [1:10:19<00:00, 21.10s/it]\n"
     ]
    }
   ],
   "source": [
    "binnum = 1\n",
    "res = {'noshear': np.zeros((binnum, 2)), 'num_noshear': np.zeros((binnum, 2)), \n",
    "       '1p': np.zeros((binnum, 2)), 'num_1p': np.zeros((binnum, 2)), \n",
    "       '1m': np.zeros((binnum, 2)), 'num_1m': np.zeros((binnum, 2)),\n",
    "       '2p': np.zeros((binnum, 2)), 'num_2p': np.zeros((binnum, 2)),\n",
    "       '2m': np.zeros((binnum, 2)), 'num_2m': np.zeros((binnum, 2))}\n",
    "\n",
    "for fname in tqdm(mdet_files):\n",
    "    \n",
    "    try:\n",
    "        d = fio.read(fname)\n",
    "    except:\n",
    "        print('this file cannot be read', fname)\n",
    "        continue\n",
    "    \n",
    "    msk = mdet.make_mdet_cuts(d, 3) # the second argument is ID of the version of the cuts. For Y6A2_METADETECT_V3, use '2'. For Y6A2_METADETECT_V4, use '3'. \n",
    "    d = d[msk]\n",
    "    \n",
    "    # shear_wgt = _find_shear_weight(d, wgt_dict, snmin, snmax, sizemin, sizemax, steps, mdet_mom) -> If the shear weight is defined by the bins of S/N and size ratio. \n",
    "    # shear_wgt = np.ones(len(d)) -> For the unweighted shear. \n",
    "    shear_wgt = 1/(0.17**2 + 0.5*(d[mdet_mom+'_g_cov_1_1'] + d[mdet_mom+'_g_cov_2_2']))\n",
    "    res = _accum_shear_per_tile(res, d['mdet_step'], d[mdet_mom+'_g_1'], d[mdet_mom+'_g_2'], shear_wgt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2c00f54-8229-4e65-880c-8ec68d73e538",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2813003824884869 0.2819703494270017 0.2816353659577443\n",
      "4.4005294846831205e-06 7.919873190585677e-05 4.1799630695269946e-05\n"
     ]
    }
   ],
   "source": [
    "# computing shear response over all the catalogs.\n",
    "g1 = res['noshear'][0][0] / res['num_noshear'][0][0]\n",
    "g1p = res['1p'][0][0] / res['num_1p'][0][0]\n",
    "g1m = res['1m'][0][0] / res['num_1m'][0][0]\n",
    "R11 = (g1p - g1m) / 2 / 0.01\n",
    "\n",
    "g2 = res['noshear'][0][1] / res['num_noshear'][0][1]\n",
    "g2p = res['2p'][0][1] / res['num_2p'][0][1]\n",
    "g2m = res['2m'][0][1] / res['num_2m'][0][1]\n",
    "R22 = (g2p - g2m) / 2 / 0.01\n",
    "\n",
    "R = (R11+R22)/2\n",
    "print(R11, R22, R)\n",
    "\n",
    "# off-diagonal part\n",
    "g2p_g1 = res['2p'][0][0] / res['num_2p'][0][0]\n",
    "g2m_g1 = res['2m'][0][0] / res['num_2m'][0][0]\n",
    "g1p_g2 = res['1p'][0][1] / res['num_1p'][0][1]\n",
    "g1m_g2 = res['1m'][0][1] / res['num_1m'][0][1]\n",
    "R12 = (g2p_g1 - g2m_g1)/2/0.01\n",
    "R21 = (g1p_g2 - g1m_g2)/2/0.01\n",
    "print(R12, R21, (R12+R21)/2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e2ce99-af9b-4022-befd-10ee50a6a508",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Combine individual catalogs for your own analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06625594-1c6a-4f31-a3f6-094289d364f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 68/200 [26:30<49:17, 22.40s/it]  "
     ]
    }
   ],
   "source": [
    "# This is where all the objects after selections are concatenated in an array.\n",
    "obj_list = []\n",
    "for fname in tqdm(mdet_files):\n",
    "    \n",
    "    try:\n",
    "        # Read in a file. \n",
    "        d = fio.read(fname)\n",
    "        # Call the external function from https://github.com/des-science/des-y6utils to apply selection cuts.\n",
    "        # Please check the github page for further details on what kinds of cuts are applied. \n",
    "        # The second argument is ID of the version of the cuts. the most recent ID is 2. \n",
    "        msk = mdet.make_mdet_cuts(d, 2) \n",
    "        # Also select objects in unsheared images.\n",
    "        noshear_mask = (d['mdet_step'] == 'noshear')\n",
    "        d = d[msk & noshear_mask]\n",
    "    except:\n",
    "        continue\n",
    "    \n",
    "    # If you'd need less or more columns (e.g., shape variance), you'd need to remove or add them here. \n",
    "    mdet_obj = np.zeros(len(d), dtype=[('ra', 'f8'), ('dec', 'f8'), ('s2n', 'f8'), ('g1', 'f8'), ('g2', 'f8'), ('R_all', 'f8'), ('R_w', 'f8'), ('w', 'f8'), ('band_flux_g', 'f8'), ('band_flux_r', 'f8'), ('band_flux_i', 'f8'), ('band_flux_z', 'f8'), ('band_flux_err_g', 'f8'), ('band_flux_err_r', 'f8'), ('band_flux_err_i', 'f8'), ('band_flux_err_z', 'f8')])\n",
    "    # Based on each object's S/N and size ratio, shear weight is decided. \n",
    "    shear_wgt = _find_shear_weight(d, wgt_dict, snmin, snmax, sizemin, sizemax, steps, mdet_mom)\n",
    "    \n",
    "    mdet_obj['ra'] = d['ra']\n",
    "    mdet_obj['dec'] = d['dec']\n",
    "    mdet_obj['s2n'] = d[mdet_mom+'_s2n']\n",
    "    mdet_obj['g1'] = d[mdet_mom+'_g_1'] # raw shear (uncorrected for the response)\n",
    "    mdet_obj['g2'] = d[mdet_mom+'_g_2'] # raw shear (uncorrected for the response)\n",
    "    mdet_obj['R_all'] = R # global shear response computed in previous cells in this notebook.\n",
    "    \n",
    "    mdet_obj['R_w'] = shear_response # shear response computed in the bins of S/N and size ratio.\n",
    "    mdet_obj['w'] = shear_wgt # shear weight computed in the bins of S/N and size ratio. \n",
    "    \n",
    "    ## NOTE: WE RECOMMEND USING PGAUSS_BAND_FLUX FOR FLUX MEASUREMENT, SINCE PGAUSS IS A PRE-PSF MEASUREMENT. \n",
    "    mdet_obj['band_flux_g'] = d['pgauss_band_flux_g']\n",
    "    mdet_obj['band_flux_r'] = d['pgauss_band_flux_r']\n",
    "    mdet_obj['band_flux_i'] = d['pgauss_band_flux_i']\n",
    "    mdet_obj['band_flux_z'] = d['pgauss_band_flux_z']\n",
    "    mdet_obj['band_flux_err_g'] = d['pgauss_band_flux_err_g']\n",
    "    mdet_obj['band_flux_err_r'] = d['pgauss_band_flux_err_r']\n",
    "    mdet_obj['band_flux_err_i'] = d['pgauss_band_flux_err_i']\n",
    "    mdet_obj['band_flux_err_z'] = d['pgauss_band_flux_err_z']\n",
    "    \n",
    "    obj_list.append(mdet_obj)\n",
    "\n",
    "mdet_all = np.concatenate(obj_list)\n",
    "# save the array if you'd like, although this might cause memory issue. \n",
    "# fio.write('metadetection_v1.fits', mdet_all)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8201cc7f-3c5b-40d8-9c24-091963859dd4",
   "metadata": {},
   "source": [
    "If you wish to use a combined catalog which includes these columns (RA, DEC, S/N, g1, g2, g1_cov, g2_cov, R, w), this is already produced and is at \"/global/cfs/cdirs/des/y6-shear-catalogs/metadetection_v2_s2n_sizer_weight.fits\". Please note that the shear in thsi catalog is from the weighted moments measurement and I used cut ID=2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e329b1f8-1bc3-4922-b5b3-a5da2eeafecb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eastlake-dev",
   "language": "python",
   "name": "eastlake-dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
